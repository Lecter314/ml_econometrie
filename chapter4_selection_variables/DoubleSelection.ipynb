{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Selection et Sample-splitting, simulations\n",
    "\n",
    "Date: 4 janvier 2018\n",
    "\n",
    "@author: jeremlhour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "\n",
    "##############################\n",
    "##############################\n",
    "### PACKAGES AND FUNCTIONS ###\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "library(\"ggplot2\")\n",
    "library(\"gridExtra\")\n",
    "library(\"glmnet\")\n",
    "library(\"MASS\")\n",
    "\n",
    "### Load user-defined functions\n",
    "source(\"src/DataSim.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(99999)\n",
    "\n",
    "R = 100 # nb simulations\n",
    "n = 200   # sample size\n",
    "p = 300   # nb variables\n",
    "K = 5     # nb folds\n",
    "tau = .5  # Treatment effect\n",
    "\n",
    "split = runif(n)\n",
    "cvgroup = as.numeric(cut(split,quantile(split,probs = seq(0, 1, 1/K)),include.lowest = T))  \n",
    "\n",
    "g = .1/log(max(p,n))\n",
    "lambda = 1.1*qnorm(1-.5*g/p)/sqrt(n) # (theoretical) Lasso penalty level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate = matrix(ncol=3, nrow=R)\n",
    "standard_error = matrix(ncol=3, nrow=R)\n",
    "t_start = Sys.time()\n",
    "pb = txtProgressBar(min = 0, max = R, initial = 0) \n",
    "\n",
    "for(r in 1:R){\n",
    "  ### GENERATION DES DONNEES\n",
    "  data = DataSim(n=n, p=p, Ry=.1, Rd=.8, Intercept=F, a=tau)\n",
    "  X = data$X; y = data$y; d = data$d\n",
    "  \n",
    "  ### METHOD 1: Naive selection\n",
    "  phi = rep(1,p+1); phi[1] = 0 # Do not penalize \"d\"\n",
    "  lasso.selec = glmnet(cbind(d,X),y, family=\"gaussian\",alpha=1,penalty.factor=phi,lambda=lambda)\n",
    "  b.lasso = coef(lasso.selec); b.lasso = b.lasso[-c(1,2)] # on enleve la constante et la variable de traitement\n",
    "  s.hat = which(b.lasso != 0)\n",
    "  if(length(s.hat)==0){\n",
    "    naive.fit = lm(y ~ d)\n",
    "  } else {\n",
    "    naive.fit = lm(y ~ d + X[,s.hat])\n",
    "  }\n",
    "  \n",
    "  ### METHOD 2: Double-Selection, no sample-splitting\n",
    "  # A. Selection sur le Traitement\n",
    "  treat.selec = glmnet(X,d, family=\"gaussian\",alpha=1,lambda=lambda)\n",
    "  b.treat = coef(treat.selec); b.treat = b.treat[-1] # on enleve la constante\n",
    "  S.d = which(b.treat != 0)\n",
    "  \n",
    "  # B. Selection sur l'Outcome\n",
    "  outcome.selec = glmnet(X,y, family=\"gaussian\",alpha=1,lambda=lambda)\n",
    "  b.outcome = coef(outcome.selec); b.outcome = b.outcome[-1] # on enleve la constante\n",
    "  S.y = which(b.outcome != 0)\n",
    "  \n",
    "  # C. Calcul de l'estimateur de double selection\n",
    "  s.hat = union(S.y,S.d)\n",
    "  if(length(s.hat)==0){\n",
    "    DS.fit = lm(y ~ d)\n",
    "  } else {\n",
    "    DS.fit = lm(y ~ d + X[,s.hat])\n",
    "  }\n",
    "  \n",
    "  # D. Compute sd\n",
    "  if(length(s.hat)==0){\n",
    "    treat.fit = lm(d ~ 1)\n",
    "  } else {\n",
    "    treat.fit = lm(d ~ X[,s.hat])\n",
    "  }\n",
    "  sigmaNum = sum(treat.fit$residuals^2*DS.fit$residuals^2) /(n - length(s.hat) - 1)\n",
    "  sigmaDenom = sum(treat.fit$residuals^2) / n\n",
    "  stdev_dbs = sqrt( sigmaNum / sigmaDenom^2) / sqrt(n)\n",
    "  \n",
    "  \n",
    "  ### METHOD 3: Double selection et sample-splitting\n",
    "  theta = vector(length=K)\n",
    "  sigma = matrix(nrow=K,ncol=2)\n",
    "  for(k in 1:K){\n",
    "    Ik  = (cvgroup==k) # separer les donnees\n",
    "    NIk = (cvgroup!=k)\n",
    "    \n",
    "    # 0. Ajustement du lambda\n",
    "    gstar = .1/log(max(p,sum(NIk)))\n",
    "    lambdastar = 1.1*qnorm(1-.5*g/p)/sqrt(sum(NIk)) # Lasso penalty level\n",
    "    \n",
    "    # Abis. Selection sur le traitement\n",
    "    treat.selec = glmnet(X[NIk,],d[NIk], family=\"gaussian\",alpha=1,lambda=lambdastar)\n",
    "    b.treat = coef(treat.selec); b.treat = b.treat[-1] # on enleve la constante\n",
    "    S.d = which(b.treat != 0)  \n",
    "    \n",
    "    # Bbis. Selection on Outcome\n",
    "    outcome.selec = glmnet(X[NIk,],y[NIk], family=\"gaussian\",alpha=1,lambda=lambdastar)\n",
    "    b.outcome = coef(outcome.selec); b.outcome = b.outcome[-1] # on enleve la constante\n",
    "    S.y = which(b.outcome != 0)\n",
    "    \n",
    "    # Cbis. Compute Post-Double-Selection\n",
    "    s.hat = union(S.y,S.d)\n",
    "    if(length(s.hat)==0){\n",
    "      outcomePL = lm(y[NIk] ~ 1)\n",
    "      treatPL = lm(d[NIk] ~ 1)\n",
    "    } else {\n",
    "      outcomePL = lm(y[NIk] ~ X[NIk,s.hat])\n",
    "      treatPL = lm(d[NIk] ~ X[NIk,s.hat])\n",
    "    }\n",
    "    \n",
    "    # D. Target param on left-out sample\n",
    "    ytilde = y[Ik] - cbind(rep(1,sum(Ik)),X[Ik,s.hat])%*%coef(outcomePL)\n",
    "    dtilde = d[Ik] - cbind(rep(1,sum(Ik)),X[Ik,s.hat])%*%coef(treatPL)\n",
    "    Ikfit = lm(ytilde ~ dtilde)\n",
    "    \n",
    "    theta[k] = Ikfit$coef['dtilde']\n",
    "    sigma[k,] = c(sum(dtilde^2*ytilde^2),\n",
    "                  sum(dtilde^2))\n",
    "  }\n",
    "  # E. Compute standard error for sample-splitting\n",
    "  sigmaNum = sum(sigma[,1]) / n\n",
    "  sigmaDenom = sum(sigma[,2]) / n\n",
    "  stdev_dbs_ss = sqrt(sigmaNum / sigmaDenom^2) / sqrt(n)\n",
    "  \n",
    "  \n",
    "  ### COLLECTING RESULTS\n",
    "  estimate[r,] = c(naive.fit$coef['d'],\n",
    "                   DS.fit$coef['d'],\n",
    "                   mean(theta))\n",
    "  standard_error[r,] =  c(summary(naive.fit)$coefficients['d',\"Std. Error\"],\n",
    "                          stdev_dbs,\n",
    "                          stdev_dbs_ss)\n",
    "  \n",
    "  setTxtProgressBar(pb,r)\n",
    "}\n",
    "close(pb)\n",
    "print(Sys.time()-t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatDisplay = data.frame()\n",
    "StatDisplay[1:3, \"Bias\"] = apply(estimate-tau,2,mean)\n",
    "StatDisplay[1:3, \"RMSE\"] = sqrt(apply((estimate-tau)^2,2,mean))\n",
    "borne_sup = estimate + qnorm(.975) * standard_error - tau\n",
    "borne_inf = estimate - qnorm(.975) * standard_error - tau\n",
    "StatDisplay[1:3, \"Coverage rate\"] = apply((borne_sup>0)*(borne_inf<0),2,mean,na.rm=T)\n",
    "\n",
    "row.names(StatDisplay) = c(\"Naive\", \"Immunized\", \"Immunized, Cross-fitted\")\n",
    "print(round(StatDisplay, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = c(mapply(function(x) rep(x,R),1:3))\n",
    "val = c(estimate)-tau\n",
    "data_res = data.frame(val = val, model = id)\n",
    "\n",
    "M = max(abs(quantile(estimate,.01,na.rm=T)),abs(quantile(estimate,.99,na.rm=T)))\n",
    "lb = -1.1*M; ub = 1.1*M\n",
    "\n",
    "get.plot <- function(data,modelS,title=\"A Title\",s){\n",
    "  plot_res <- ggplot(subset(data, (model==modelS)), aes(x=val)) + \n",
    "    geom_histogram(binwidth = .02, alpha=1, position='identity',fill=\"grey\", aes(y = ..density..)) +\n",
    "    scale_x_continuous(limits=c(lb,ub), name=\"\") +\n",
    "    ggtitle(title) + \n",
    "    stat_function(fun = dnorm, args=list(mean=0, sd=s), colour=\"black\", size=0.5, linetype = \"dashed\") +\n",
    "    theme(\n",
    "      panel.grid.major = element_blank(),\n",
    "      panel.grid.minor = element_blank(),\n",
    "      panel.background = element_blank(),\n",
    "      plot.title = element_text(lineheight=.8, face=\"bold\"),\n",
    "      legend.position=\"none\",\n",
    "      axis.text=element_text(size=12)\n",
    "    )\n",
    "  return(plot_res)\n",
    "}\n",
    "              \n",
    "options(repr.plot.width=20, repr.plot.height=8)\n",
    "grid.arrange(\n",
    "    get.plot(data_res,1,\"Naive Post-Selec\", mean(standard_error[,2])),\n",
    "    get.plot(data_res,2,\"Double-Selec\", mean(standard_error[,2])),\n",
    "    get.plot(data_res,3,\"Double-Selec, Cross-fitting\", mean(standard_error[,2])),\n",
    "    ncol=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg(\"plots/double-selec-simulations.jpg\", width=1000, height=400)\n",
    "grid.arrange(\n",
    "    get.plot(data_res,1,\"Naive Post-Selec\", mean(standard_error[,2])),\n",
    "    get.plot(data_res,2,\"Double-Selec\", mean(standard_error[,2])),\n",
    "    get.plot(data_res,3,\"Double-Selec, Cross-fitting\", mean(standard_error[,2])),\n",
    "    ncol=3\n",
    ")\n",
    "dev.off()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
